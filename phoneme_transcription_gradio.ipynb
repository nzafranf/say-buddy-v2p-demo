{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "864b7c3e",
   "metadata": {},
   "source": [
    "# Real-time Phoneme Transcription with Gradio\n",
    "This notebook demonstrates real-time phoneme transcription using a pre-trained Wav2Vec2 model and Gradio. You can use your microphone or upload an audio file to see the phoneme transcription in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88467661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from io import BytesIO\n",
    "from Levenshtein import distance as levenshtein_distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b45639",
   "metadata": {},
   "source": [
    "## Load Pre-trained Phoneme Transcription Model\n",
    "We use a Wav2Vec2 model fine-tuned for phoneme recognition. The model and processor are loaded below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7ee65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the phoneme model and processor\n",
    "def load_phoneme_model():\n",
    "    processor = Wav2Vec2Processor.from_pretrained(\"vitouphy/wav2vec2-xls-r-300m-timit-phoneme\")\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\"vitouphy/wav2vec2-xls-r-300m-timit-phoneme\")\n",
    "    return processor, model\n",
    "\n",
    "processor, model = load_phoneme_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5252bc18",
   "metadata": {},
   "source": [
    "## Define Real-time Audio Processing Function\n",
    "This function takes audio input, splits it into chunks, transcribes phonemes for each chunk, and returns the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d71e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phoneme scoring and chunking logic from main.py\n",
    "vowel_tolerance = {\n",
    "    \"a\": {\"a\", \"É‘\", \"É™\", \"Ã¦\"},\n",
    "    \"e\": {\"e\", \"É›\", \"É™\"},\n",
    "    \"i\": {\"i\", \"Éª\"},\n",
    "    \"o\": {\"o\", \"É”\"},\n",
    "    \"u\": {\"u\", \"ÊŠ\"}\n",
    "}\n",
    "\n",
    "def split_audio_chunks(y, sr, chunk_duration=1.2):\n",
    "    chunk_samples = int(chunk_duration * sr)\n",
    "    total_samples = len(y)\n",
    "    chunks = []\n",
    "    for start in range(0, total_samples, chunk_samples):\n",
    "        end = min(start + chunk_samples, total_samples)\n",
    "        chunks.append(y[start:end])\n",
    "    return chunks\n",
    "\n",
    "def phoneme_score(audio_bytes, target_word=\"masyarakat\"):\n",
    "    # Use global processor, model\n",
    "    global processor, model\n",
    "    y, sr = librosa.load(BytesIO(audio_bytes), sr=16000)\n",
    "    input_values = processor(y, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**input_values).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    phonemes = processor.batch_decode(predicted_ids)[0].split()\n",
    "    target_phonemes = \"m a Êƒ a É¾ a k a t\".split()\n",
    "    child_phonemes = phonemes\n",
    "    tolerant_child = []\n",
    "    for ph in child_phonemes:\n",
    "        if any(ph in vowel_tolerance.get(v, set()) for v in vowel_tolerance):\n",
    "            for v, variants in vowel_tolerance.items():\n",
    "                if ph in variants:\n",
    "                    tolerant_child.append(v)\n",
    "                    break\n",
    "        else:\n",
    "            tolerant_child.append(ph)\n",
    "    edit_distance = levenshtein_distance(\"\".join(target_phonemes), \"\".join(tolerant_child))\n",
    "    max_distance = max(len(target_phonemes), len(tolerant_child))\n",
    "    score = ((max_distance - edit_distance) / max_distance) * 100 if max_distance > 0 else 0\n",
    "    mismatches = []\n",
    "    for i, (target, child) in enumerate(zip(target_phonemes, tolerant_child + [\"\"] * (len(target_phonemes) - len(tolerant_child)))):\n",
    "        if i >= len(tolerant_child) or (target != child and not (target in vowel_tolerance and child in vowel_tolerance.get(target, set()))):\n",
    "            mismatches.append(f\"Pos {i+1}: {target} â†’ {child if i < len(tolerant_child) else 'missing'}\")\n",
    "    return {\n",
    "        \"phonemes\": \" \".join(phonemes),\n",
    "        \"score\": score,\n",
    "        \"mismatches\": mismatches,\n",
    "        \"target_phonemes\": \" \".join(target_phonemes),\n",
    "        \"tolerant_child\": \" \".join(tolerant_child)\n",
    "    }\n",
    "\n",
    "def process_audio(audio, chunk_duration=1.2):\n",
    "    if isinstance(audio, tuple):\n",
    "        audio = audio[0]  # gradio mic returns (np.array, sr)\n",
    "    if isinstance(audio, np.ndarray):\n",
    "        # Convert to bytes\n",
    "        buf = BytesIO()\n",
    "        sf.write(buf, audio, 16000, format='WAV')\n",
    "        audio_bytes = buf.getvalue()\n",
    "    else:\n",
    "        audio_bytes = audio\n",
    "    y, sr = librosa.load(BytesIO(audio_bytes), sr=16000)\n",
    "    chunks = split_audio_chunks(y, sr, chunk_duration)\n",
    "    results = []\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        if len(chunk) == 0:\n",
    "            continue\n",
    "        chunk_bytes = BytesIO()\n",
    "        sf.write(chunk_bytes, chunk, sr, format='WAV')\n",
    "        chunk_bytes.seek(0)\n",
    "        chunk_bytes_data = chunk_bytes.read()\n",
    "        result = phoneme_score(chunk_bytes_data)\n",
    "        results.append({\n",
    "            \"chunk\": idx+1,\n",
    "            \"start\": idx*chunk_duration,\n",
    "            \"end\": (idx+1)*chunk_duration,\n",
    "            **result\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00020ba8",
   "metadata": {},
   "source": [
    "## Create Gradio Interface for Real-time Phoneme Transcription\n",
    "We will now set up a Gradio interface that accepts live audio input and displays the phoneme transcription output for each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675ef4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio interface for real-time phoneme transcription\n",
    "def gradio_transcribe(audio):\n",
    "    results = process_audio(audio)\n",
    "    display = \"\"\n",
    "    for r in results:\n",
    "        display += f\"<b>Chunk {r['chunk']} ({r['start']:.1f}-{r['end']:.1f}s)</b><br>\"\n",
    "        display += f\"<b>Target Phonemes:</b> <code>{r['target_phonemes']}</code><br>\"\n",
    "        display += f\"<b>Detected Phonemes:</b> <code>{r['tolerant_child']}</code><br>\"\n",
    "        display += f\"<b>Raw Model Output:</b> <code>{r['phonemes']}</code><br>\"\n",
    "        display += f\"<b>Score:</b> <code>{r['score']:.1f}%</code><br>\"\n",
    "        if r[\"mismatches\"]:\n",
    "            display += f\"<span style='color:red'><b>Missed/Incorrect Phonemes:</b><br>{'<br>'.join(r['mismatches'])}</span><br>\"\n",
    "        else:\n",
    "            display += f\"<span style='color:green'><b>All phonemes correct! ðŸŽ‰</b></span><br>\"\n",
    "        display += \"<hr>\"\n",
    "    return display\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=gradio_transcribe,\n",
    "    inputs=gr.Audio(source=\"microphone\", type=\"filepath\", label=\"Speak or Upload Audio (WAV)\"),\n",
    "    outputs=gr.outputs.HTML(label=\"Phoneme Transcription Results\"),\n",
    "    title=\"Real-time Phoneme Transcription\",\n",
    "    description=\"Speak or upload a WAV file to see real-time phoneme transcription and scoring.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10676b04",
   "metadata": {},
   "source": [
    "## Launch Gradio App\n",
    "Run the cell below to launch the Gradio app and start real-time phoneme transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538a081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the Gradio app\n",
    "iface.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
